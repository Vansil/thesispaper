\newpage
\section{Background}
% max 10 pages

\subsection{Modelling Framework}

\subsubsection{Structural Causal Model}
Throughout this thesis we will assume that the data is generated by a Structural Causal Model (SCM). This modelling framework is widely used in the field of causality, and is still very flexible. 

A distinction is made between endogenous variables and exogenous variables. Endogenous variables are known, either by measurement (data $\B{X}$) or method design (e.g. context variables $\B{C}$ in LCD). They are represented by an index set $\C{I}$. Exogenous variables are latent, a typical example is noise variables $\B{N}$. Exogenous variables are represented by an index set $\C{J}$.

The causal mechanism $\B{f}$ of a SCM is a function that describes how all variables relate to each other. It maps a product space of all variables to a product space of the endogenous variables. 

The components of the causal mechanism $f_i$ usually do not depend on all variables, but rather on a small subset that we call the parents of variable $X_i$. The augmented graph $\C{H}_{\C{M}}$ represents these child-parent relations with directed edges between variable nodes.

Lastly, since the values of the exogenous variables are not known, they are modelled with a product probability measure $\Prb_{\BC{E}}$. Data can then be sampled from a fully defined SCM by sampling from this measure and (iteratively) applying the functions $f_i$ to compute the values of the endogenous variables.

Usually, a more informal definition of SCMs suffices, consisting of the structural equations of the endogenous variables and the density function of the exogenous variables, indicated below with $\B{X}$ and $\B{E}$ respectively:

$$\C{M}: \begin{cases}
    X_i &= f_i(\B{X}_{\pasub{}{i} \cap \C{I}}, \B{E}_{\pasub{}{i} \cap \C{J}}) \\
    p_{\B{E}} & = \prod_{j\in\C{J}} p_{E_j}.
\end{cases}$$

In a practical setting we are unable to infer the real structure of the exogenous variables. A more practical graphical representation of a SCM is the graph $\C{G}_{\C{M}}$, which is an abstraction of the augmented graph $\C{H}_{\C{M}}$. The nodes of this graph are only the endogenous variables. The relations among endogenous variables are still represented with directed edges ($i \to j$). Variables that are confounded by an exogenous variable (i.e. share an exogenous ancestor) are connected by a bidirected edge\footnote{The same representation of confounding is used when we marginalize over a subset of endogenous variables} ($i\oto j$).

\subsubsection{Causal Assumptions and Interventional Data}
If one observes two variables $X$ and $Y$, and measures a dependence among them, it is impossible to say if $X$ causes $Y$ or the other way around. More formally, one cannot infer the causal direction from a probability measure $\Prb_{\{X,Y\}}$ alone. This is why we have to rely on what \citet{pearl2009causality} calls causal assumptions. There are many assumptions that can be made, and some will be discussed in Section \ref{sec:back:prin}, along with inference methods that rely on them in Section \ref{sec:back:meth}. 

One assumption particularly relevant to this thesis is related to the method of data acquisition. A distinction is made between observational data and interventional data. Observational data is gathered without interference in the system. We assume that there is an underlying SCM, and every data point is a sample from it. The sampling distribution approximates the observational distribution $\Prb_{\B{X}}$, and it is theoretically impossible to infer any causal statements without further assumptions. 

Interventional data is gathered while we interfere with the system. Every data point is measured while an intervention is performed. Formally, this intervention is a manipulation of the causal mechanism, which induces a new SCM in a way that is commonly restricted by assumptions. This might render the causal inference problem theoretically possible. 

A concrete example is the perfect intervention. A perfect intervention sets a variable $X_i$ to a fixed value $\xi_i$, denoted as $\intervene(X_i=\xi_i)$. This removes all the dependence of $X_i$ on its parents $\pasub{\C{H}}{i}$. The adapted SCM induces a different, interventional distribution $\Prb_{\B{X}|\intervene(X_i=\xi_i)}$. \citet{pearl2009causality} developed a do-calculus that can be used to make causal inferences from observational and intervenional data. As a simple example, take a system of two variables that are related as $X\to Y$. From the observational data we only know that $X$ and $Y$ are dependent. However, if we also have access to distributions $\Prb_{\{X,Y\}|\intervene(X=x)}$ and $\Prb_{\{X,Y\}|\intervene(Y=y)}$ we can see that intervening on $Y$ does not affect $X$, whereas intervening on $X$ does affect $Y$. We conclude that $X$ causes $Y$. 


\subsubsection{Markov Property}
The Markov Property is a very common assumption that links the SCM to conditional independence relations (CIRs). The property follows from the definition of the SCM, so it does not add a restriction to our modelling. 

The notion of d-separation is used to infer CIRs. We say that two variables $X$ and $Y$ are d-separated by a conditioning set of variables $\B{C}$, if all walks from $X$ to $Y$ are d-blocked by $\B{C}$. This is denoted as $\dsep{X}{Y}{\B{C}}{\C{G}}$. On each walk we will consider if the variables are a collider, that is: if both edges of the walk point towards it ($...\to Z \ot ...$). A walk is d-blocked in three cases:

\begin{compactenum}
    \item $X$ or $Y$ are in $\B{C}$
    \item The walk contains a non-collider $Z$ that is in $\B{C}$
    \item The walk contains a collider $Z$ that is not in $\B{C}$, nor any of its descendents
\end{compactenum}

Consider the graph in Figure \ref{fig:2:dsep}. By case 1, $X_1$ blocks the walk from $X_1$ to $X_3$. $X_2$ blocks this walk if it is in the conditioning set $\B{C}$ by case 2. According to case 3, the walk from $X_2$ to $X_4$ is blocked if neither $X_3$ nor $X_5$ are in $\B{C}$.

\begin{figure}[h]
    \centering
    \includegraphics[width=.3\textwidth]{2dsep.jpg}
    \caption{Example graph}
    \label{fig:2:dsep}
\end{figure}

The Global Markov Property links d-separation to conditional independence:

$$\dsep{\B{A}}{\B{B}}{\B{C}}{\C{G}} \implies \indep{\B{A}}{\B{B}}{\B{C}}{\Prb_{\B{X}}}$$

The Markov Property with d-separation is only valid for SCMs with an acyclic graph\footnote{It is also valid for some restricted cases of cyclic SCMs, cf. \citet{forre2017markov}}. However, a generalization of d-separation was developed by \citet{forre2017markov} that applies to the cyclic case as well.

Markov equiv


% sigma separation: small adaptation to generalize to cyclic


% Follows from SCM definition

% * d-separation (visual graph example)
% * (conditional) independence: in terms of prob.
% * Markov property
% * Markov equivalence (covered edge reversal, immortalities?)


\subsection{Principles of Causal Inference}
\label{sec:back:prin}

Assumptions are a restriction on the form of the SCM.


\input{23relatedwork.tex}
\label{sec:back:meth}